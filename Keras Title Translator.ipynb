{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport keras\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom nltk.translate.bleu_score import corpus_bleu\ndata = spark.read.csv('FileStore/tables/cleaned_translations.csv', header = True)\ndata = data.toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1249521954556919&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansigreen\">from</span> sklearn<span class=\"ansiyellow\">.</span>model_selection <span class=\"ansigreen\">import</span> train_test_split<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> <span class=\"ansigreen\">from</span> nltk<span class=\"ansiyellow\">.</span>translate<span class=\"ansiyellow\">.</span>bleu_score <span class=\"ansigreen\">import</span> corpus_bleu<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 6</span><span class=\"ansiyellow\"> </span>data <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>csv<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;FileStore/tables/cleaned_translations.csv&apos;</span><span class=\"ansiyellow\">,</span> header <span class=\"ansiyellow\">=</span> <span class=\"ansigreen\">True</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      7</span> data <span class=\"ansiyellow\">=</span> data<span class=\"ansiyellow\">.</span>limit<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">5000</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      8</span> data <span class=\"ansiyellow\">=</span> data<span class=\"ansiyellow\">.</span>toPandas<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansicyan\">csv</span><span class=\"ansiblue\">(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)</span>\n<span class=\"ansigreen\">    470</span>             path <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>path<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    471</span>         <span class=\"ansigreen\">if</span> type<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">==</span> list<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 472</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_df<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jreader<span class=\"ansiyellow\">.</span>csv<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_spark<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonUtils<span class=\"ansiyellow\">.</span>toSeq<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    473</span>         <span class=\"ansigreen\">elif</span> isinstance<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">,</span> RDD<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    474</span>             <span class=\"ansigreen\">def</span> func<span class=\"ansiyellow\">(</span>iterator<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o2599.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 177, localhost, executor driver): org.apache.spark.SparkException: Process List(/local_disk0/pythonVirtualEnvDirs/virtualEnv-235ed83d-c4e6-4950-b664-b47942c6652f/bin/python, /local_disk0/pythonVirtualEnvDirs/virtualEnv-235ed83d-c4e6-4950-b664-b47942c6652f/bin/pip, install, bleu_score, --disable-pip-version-check) exited with code 1. Error:   Could not find a version that satisfies the requirement bleu_score (from versions: )\nNo matching distribution found for bleu_score\n\n\tat org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:1325)\n\tat org.apache.spark.util.Utils$.installLibrary(Utils.scala:807)\n\tat org.apache.spark.executor.Executor$$anonfun$updateDependencies$3.apply(Executor.scala:898)\n\tat org.apache.spark.executor.Executor$$anonfun$updateDependencies$3.apply(Executor.scala:886)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.updateDependencies(Executor.scala:886)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:462)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2100)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2088)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2087)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2087)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1076)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1076)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1076)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2319)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2267)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2255)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:873)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2252)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:259)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:269)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:69)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:75)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:497)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectResult(Dataset.scala:2827)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3439)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2556)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2556)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:3423)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:228)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:85)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:158)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3422)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2556)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2770)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:244)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:69)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:196)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:196)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:415)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:298)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:707)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Process List(/local_disk0/pythonVirtualEnvDirs/virtualEnv-235ed83d-c4e6-4950-b664-b47942c6652f/bin/python, /local_disk0/pythonVirtualEnvDirs/virtualEnv-235ed83d-c4e6-4950-b664-b47942c6652f/bin/pip, install, bleu_score, --disable-pip-version-check) exited with code 1. Error:   Could not find a version that satisfies the requirement bleu_score (from versions: )\nNo matching distribution found for bleu_score\n\n\tat org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:1325)\n\tat org.apache.spark.util.Utils$.installLibrary(Utils.scala:807)\n\tat org.apache.spark.executor.Executor$$anonfun$updateDependencies$3.apply(Executor.scala:898)\n\tat org.apache.spark.executor.Executor$$anonfun$updateDependencies$3.apply(Executor.scala:886)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.updateDependencies(Executor.scala:886)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:462)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["def clean_train_test(data, train_perc, x, y, limit = -1):\n#function to drop nones, get subset of translation pairs if necessary\n#data: pd dataframe of translation pairs, train_perc: percent to train on, remainder will be test set, x: name of source language column, y: name of target language column, limit: number of rows from dataframe to keep, -1 = all\n  #limit\n  if limit == -1:\n    limit = len(data)\n  data = data.iloc[:limit,:]\n  #drop nones\n  data = data.loc[~data.applymap(lambda x: x is None).iloc[:,2],:]\n  data = data.loc[~data.applymap(lambda x: x is None).iloc[:,1],:].reset_index(drop=True)\n  #train test split\n  source_train, source_test, target_train, target_test = train_test_split(data[x], data[y], test_size=(1-train_perc))\n  return source_train, source_test, target_train, target_test"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["#data prep\n#create tokenizer, input pandas series\ndef create_tokenizer(sentences):\n  sentences = list(sentences)\n  tokenizer = keras.preprocessing.text.Tokenizer()\n  tokenizer.fit_on_texts(lines)\n  return tokenizer\n\n#get max sentence length, input pandas series\ndef max_length(sentences):\n  sentences = list(sentences)\n  return max(len(line.split()) for line in sentences)\n\n#creating tokenizers, max lengths etc.\nsource_tokenizer = create_tokenizer(source_train)\ntarget_tokenizer = create_tokenizer(target_train)\nsource_vocab_size = len(source_tokenizer.word_index) + 1\ntarget_vocab_size = len(target_tokenizer.word_index) + 1\nsource_length = max_length(source_train)\ntarget_length = max_length(target_train)\n\n#encode sequences/pad\ndef encode_sequences(tokenizer, length, lines):\n\t#integer encode sequences\n\tX = tokenizer.texts_to_sequences(lines)\n\t#pad sequences with 0 values\n\tX = keras.preprocessing.sequence.pad_sequences(X, maxlen=length, padding='post')\n\treturn X\n\n#one hot encode target sequence\ndef encode_output(sequences, vocab_size):\n\tylist = list()\n\tfor sequence in sequences:\n\t\tencoded = keras.utils.to_categorical(sequence, num_classes=vocab_size)\n\t\tylist.append(encoded)\n\ty = np.array(ylist)\n\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n\treturn y\n\n#defining the model\ndef define_model(source_vocab, target_vocab, source_timesteps, target_timesteps, n_units):\n\tmodel = keras.models.Sequential()\n\tmodel.add(keras.layers.Embedding(source_vocab, n_units, input_length=source_timesteps, mask_zero=True))\n\tmodel.add(keras.layers.LSTM(n_units))\n\tmodel.add(keras.layers.RepeatVector(target_timesteps))\n\tmodel.add(keras.layers.LSTM(n_units, return_sequences=True))\n\tmodel.add(keras.layers.TimeDistributed(keras.layers.Dense(target_vocab, activation='softmax')))\n\treturn model\n\n#model prediction/evaluation\n#map an integer to a word\ndef word_for_id(integer, tokenizer):\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == integer:\n\t\t\treturn word\n\treturn None\n \n#generate target given source sequence\ndef predict_sequence(model, tokenizer, source):\n\tprediction = model.predict(source, verbose=0)[0]\n\tintegers = [argmax(vector) for vector in prediction]\n\ttarget = list()\n\tfor i in integers:\n\t\tword = word_for_id(i, tokenizer)\n\t\tif word is None:\n\t\t\tbreak\n\t\ttarget.append(word)\n\treturn ' '.join(target)\n \n#evaluate model\ndef evaluate_model(model, tokenizer, sources, raw_dataset):\n\tactual, predicted = list(), list()\n\tfor i, source in enumerate(sources):\n\t\t# translate encoded source text\n\t\tsource = source.reshape((1, source.shape[0]))\n\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n\t\traw_target, raw_src = raw_dataset[i]\n\t\tif i < 10:\n\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n\t\tactual.append(raw_target.split())\n\t\tpredicted.append(translation.split())\n\t# calculate BLEU score\n\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["#initial cleaning\nraw_source_train, raw_source_test, raw_target_train, raw_target_test = raw_clean_train_test(data, .8, 'english', 'french', 5000)\n#prepare training data\ntrain_source = encode_sequences(source_tokenizer, source_length, source_train)\ntrain_target = encode_sequences(target_tokenizer, target_length, target_train)\ntrain_target = encode_output(train_target, english_vocab_size)\n#prepare validation data\ntest_source = encode_sequences(source_tokenizer, source_length, source_test)\ntest_target = encode_sequences(french_tokenizer, target_length, target_test)\ntest_target = encode_output(test_target, english_vocab_size)\n\n#create model\nmodel = define_model(source_vocab_size, target_vocab_size, source_length, target_length, 256)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n\n#evaluate model\nevaluate_model(model, target_tokenizer, train_source, raw_source_train)\nevaluate_model(model, target_tokenizer, test_source, raw_source_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4}],"metadata":{"name":"keras translator","notebookId":1249521954556918},"nbformat":4,"nbformat_minor":0}
